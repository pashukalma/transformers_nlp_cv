{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI9GhfnUW8tW",
        "outputId": "e721bb89-645f-4036-c61d-0367d7eb8096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.5.1+cu124', '0.20.1+cu124')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch, torchvision, torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pickle, math, random\n",
        "from copy import deepcopy\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.__version__, torchvision.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Architecture"
      ],
      "metadata": {
        "id": "PdjJCRJMw7v1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i8dJecviO8k"
      },
      "outputs": [],
      "source": [
        "''' Transformer\n",
        "tokenization, word embedding, positional encoding, attention, encoder-decoder transformer '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('english-to-french.csv')\n",
        "print(df[:3], len(df), df.iloc[47172]['en'], df.iloc[47172]['fr'])\n",
        "from transformers import XLMTokenizer\n",
        "from collections import Counter\n",
        "tokenizer = XLMTokenizer.from_pretrained('xlm-clm-enfr-1024')\n",
        "tokenized_en = tokenizer.tokenize(\"I don't speak english\")\n",
        "tokenized_fr = tokenizer.tokenize(\"Je ne parle pas francais\")\n",
        "print(tokenized_en, tokenized_fr)\n",
        "\n",
        "en = df['en'].tolist()\n",
        "en_tokens = [['BOS'] + tokenizer.tokenize(x) + ['EOS'] for x in en]\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "wordcount = Counter()\n",
        "for sentence in en_tokens:\n",
        "  for word in sentence:\n",
        "    wordcount[word] += 1\n",
        "frequency = wordcount.most_common(50000)\n",
        "total_en_words = len(frequency) +2\n",
        "en_worddict = {word[0]:idx+2 for idx, word in enumerate(frequency)}\n",
        "en_worddict['PAD'] = PAD\n",
        "en_worddict['UNK'] = UNK\n",
        "en_idxdict = {v:k for k,v in en_worddict.items()}\n",
        "en_idx = [en_worddict.get(i,UNK) for i in tokenized_en]\n",
        "print(en_idx)\n",
        "entokens = [en_idxdict.get(i, 'UNK') for i in en_idx]\n",
        "print(entokens)\n",
        "en_phrase = ''.join(entokens)\n",
        "en_phrase = en_phrase.replace('</w>', ' ')\n",
        "for x in '''?:;.,'(\"-!&)%''':\n",
        "  en_phrase = en_phrase.replace(f' {x}', f'{x}')\n",
        "print(en_phrase)\n",
        "\n",
        "fr = df['fr'].tolist()\n",
        "fr_tokens = [['BOS'] + tokenizer.tokenize(x) + ['EOS'] for x in fr]\n",
        "wordcount = Counter()\n",
        "for sentence in fr_tokens:\n",
        "  for word in sentence:\n",
        "    wordcount[word] += 1\n",
        "frequency = wordcount.most_common(50000)\n",
        "total_fr_words = len(frequency) +2\n",
        "fr_worddict = {word[0]:idx+2 for idx, word in enumerate(frequency)}\n",
        "fr_worddict['PAD'] = PAD\n",
        "fr_worddict['UNK'] = UNK\n",
        "fr_idxdict = {v:k for k,v in fr_worddict.items()}\n",
        "fr_idx = [fr_worddict.get(i,UNK) for i in tokenized_fr]\n",
        "print(fr_idx)\n",
        "frtokens = [fr_idxdict.get(i, 'UNK') for i in fr_idx]\n",
        "print(frtokens)\n",
        "fr_phrase = ''.join(frtokens)\n",
        "fr_phrase = fr_phrase.replace('</w>', ' ')\n",
        "for x in '''?:;.,'(\"-!&)%''':\n",
        "  fr_phrase = fr_phrase.replace(f' {x}', f'{x}')\n",
        "print(fr_phrase)\n",
        "with open('dict.p', 'wb') as fd:\n",
        "  pickle.dump((en_worddict, en_idxdict, fr_worddict, fr_idxdict), fd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbjTX_7wxHUR",
        "outputId": "101eb0d4-4677-4e75-c569-5f3f5f9e7f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                                 en  \\\n",
            "0           0  Two young, White males are outside near many b...   \n",
            "1           0  Several men in hard hats are operating a giant...   \n",
            "2           0    A little girl climbing into a wooden playhouse.   \n",
            "\n",
            "                                                  fr  \n",
            "0  Deux jeunes mâles blancs se trouvent à l’extér...  \n",
            "1  Plusieurs hommes portant un chapeau d'assaut f...  \n",
            "2  Une petite fille grimpant dans une maison de j...   47173 Look both ways before you cross the stree! Regardez les deux côtés avant de traverser la tige!\n",
            "['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'eng', 'lish</w>'] ['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>']\n",
            "[15, 100, 38, 377, 227, 244]\n",
            "['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'eng', 'lish</w>']\n",
            "i don't speak english \n",
            "[28, 40, 231, 32, 726, 370]\n",
            "['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>']\n",
            "je ne parle pas francais \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' batch creation '''\n",
        "out_en_ids = [[en_worddict.get(word, 1) for word in s] for s in en_tokens]\n",
        "out_fr_ids = [[fr_worddict.get(word, 1) for word in s] for s in fr_tokens]\n",
        "sorted_ids = sorted(range(len(out_en_ids)), key=lambda x:len(out_en_ids[x]))\n",
        "out_en_ids = [out_en_ids[x] for x in sorted_ids]\n",
        "out_fr_ids = [out_fr_ids[x] for x in sorted_ids]\n",
        "batch_size = 128\n",
        "batch_index = []\n",
        "idx_lst = np.arange(0, len(en_tokens), batch_size)\n",
        "np.random.shuffle(idx_lst)\n",
        "for idx in idx_lst:\n",
        "  batch_index.append(np.arange(idx, min(len(en_tokens), idx+batch_size)))\n",
        "def seq_padding(X, padding=0):\n",
        "  L = [len(x) for x in X]\n",
        "  ML = max(L)\n",
        "  seq_padded = np.array([np.concatenate([x, [padding]*(ML -len(x))])\n",
        "          if len(x) < ML else x for x in X])\n",
        "  return seq_padded\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def subsequent_mask(size):\n",
        "  attn_shape = (1, size, size)\n",
        "  subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "  output = torch.from_numpy(subsequent_mask) ==0\n",
        "  return output\n",
        "def make_mask(tgt, pad):\n",
        "  tgt_mask = (tgt !=pad).unsqueeze(-2)\n",
        "  output = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
        "  return output\n",
        "class Batch:\n",
        "  def __init__(self, src, trg=None, pad=0):\n",
        "    src = torch.from_numpy(src).to(device).long()\n",
        "    trg = torch.from_numpy(trg).to(device).long()\n",
        "    self.src = src\n",
        "    self.src_mask = (src != pad).unsqueeze(-2)\n",
        "    if trg is not None:\n",
        "      self.trg = trg[:, :-1]\n",
        "      self.trg_y = trg[:, 1:]\n",
        "      self.trg_mask = make_mask(self.trg, pad)\n",
        "      self.ntokens = (self.trg_y !=pad).data.sum()\n",
        "batches = []\n",
        "for batch in batch_index:\n",
        "  batch_en = [out_en_ids[x] for x in batch]\n",
        "  batch_fr = [out_fr_ids[x] for x in batch]\n",
        "  batch_en = seq_padding(batch_en)\n",
        "  batch_fr = seq_padding(batch_fr)\n",
        "  batches.append(Batch(batch_en, batch_fr))"
      ],
      "metadata": {
        "id": "KgGysR70c9sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' embedding '''\n",
        "src_vocabulary = len(en_worddict)\n",
        "tgt_vocabulary = len(fr_worddict)\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, model, vocab):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab, model)\n",
        "    self.model = model\n",
        "  def forward(self, x):\n",
        "    out = self.emb(x) * math.sqrt(self.model)\n",
        "    return out"
      ],
      "metadata": {
        "id": "VBZ-Uw5hc8m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' positional encoding of sequences '''\n",
        "class PositionalEnc(nn.Module):\n",
        "  def __init__(self, model, dropout, maxlen=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    posenc = torch.zeros(maxlen, model, device=device)\n",
        "    position = torch.arange(0., maxlen, device=device).unsqueeze(1)\n",
        "    divterm = torch.exp(torch.arange(\n",
        "        0., model, 2, device=device)* -(math.log(10000.0)/model))\n",
        "    posenc_pos = torch.mul(position, divterm)\n",
        "    posenc[:, 0::2] = torch.sin(posenc_pos)\n",
        "    posenc[:, 1::2] = torch.cos(posenc_pos)\n",
        "    posenc = posenc.unsqueeze(0)\n",
        "    self.register_buffer('posenc', posenc)\n",
        "  def forward(self, x):\n",
        "    x = x + self.posenc[:, :x.size(1)].requires_grad_(False)\n",
        "    out = self.dropout(x)\n",
        "    return out\n",
        "posenc = PositionalEnc(256, 0.1)\n",
        "x = torch.zeros(1, 8, 256).to(device)\n",
        "y = posenc.forward(x)\n",
        "y"
      ],
      "metadata": {
        "id": "n6E9dS2Nc8qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd24aa81-6160-48d8-a78b-e548eee5d09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000e+00,  1.1111e+00,  0.0000e+00,  ...,  1.1111e+00,\n",
              "           0.0000e+00,  1.1111e+00],\n",
              "         [ 9.3497e-01,  6.0034e-01,  8.9107e-01,  ...,  0.0000e+00,\n",
              "           1.1940e-04,  1.1111e+00],\n",
              "         [ 0.0000e+00, -4.6239e-01,  1.0646e+00,  ...,  1.1111e+00,\n",
              "           2.3880e-04,  0.0000e+00],\n",
              "         ...,\n",
              "         [-1.0655e+00,  3.1518e-01, -1.1091e+00,  ...,  1.1111e+00,\n",
              "           5.9700e-04,  1.1111e+00],\n",
              "         [-3.1046e-01,  1.0669e+00, -7.1559e-01,  ...,  1.1111e+00,\n",
              "           7.1640e-04,  1.1111e+00],\n",
              "         [ 7.2999e-01,  8.3767e-01,  2.5419e-01,  ...,  1.1111e+00,\n",
              "           8.3581e-04,  1.1111e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mechanism"
      ],
      "metadata": {
        "id": "d6bu8HHFAJTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' The attention mechanism '''\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  d_k = query.size(-1)\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  attn_ = nn.functional.softmax(scores, dim=-1)\n",
        "  if dropout is not None:\n",
        "    attn_ = dropout(attn_)\n",
        "  return torch.matmul(attn_, value), attn_\n",
        "\n",
        "from copy import deepcopy\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, h, d_model, dropout=0.1):\n",
        "    super().__init__()\n",
        "    assert d_model % h == 0\n",
        "    self.d_k == d_model//h\n",
        "    self.h = h\n",
        "    self.linears = nn.ModuleList(\n",
        "        [deepcopy(nn.Linear(d_model, d_model)) for i in range(4)])\n",
        "    self.attn = None\n",
        "    self.dropout = nn.Dropout(p =dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(1)\n",
        "    nbatches = query.size(0)\n",
        "    query, key, value = [l(x).view(nbatches,-1, self.h, self.d_k).transpose(1,2)\n",
        "        for l, x in zip(self.linears, (query, key, value))]\n",
        "    x, self.attn = attention(\n",
        "        query, key, value, mask = mask, dropout = self.dropout)\n",
        "    x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "    output = self.linears[-1](x)\n",
        "    return output\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.w_1 = nn.Linear(d_model, d_ff)\n",
        "    self.w_2 = nn.Lineat(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    h1 = self.w_1(x)\n",
        "    h2 = self.dropout(h1)\n",
        "    return self.w_2(h2)"
      ],
      "metadata": {
        "id": "SGS2rZ5Sc8xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "eLkVf2n-AjS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self, size, dropout):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x, sublayer):\n",
        "    output = x + self.dropout(sublayer(self.norm(x)))\n",
        "    return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, fee_forward, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = nn.ModuleList([deepcopy(\n",
        "        SublayerConnection(size, dropout)) for i in range(2)])\n",
        "    self.size = size\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    output = self.sublayer[1](x, self.feed_forward)\n",
        "    return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, fee_forward, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = nn.ModuleList([deepcopy(\n",
        "        SublayerConnection(size, dropout)) for i in range(2)])\n",
        "    self.size = size\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    output = self.sublayer[1](x, self.feed_forward)\n",
        "    return output\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Paramters(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    x_score = (x -mean) / torch.sqrt(std**2 + self.eps)\n",
        "    output = self.a_2*x_score + self.b_2\n",
        "    return output"
      ],
      "metadata": {
        "id": "WSo0w6Tuc81N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder-Decoder Transformer"
      ],
      "metadata": {
        "id": "wLWDii9xKHu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self.attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = nn.ModuleList(\n",
        "        [deepcopy(SublayerConnection(size, dropout)) for i in range(3)])\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
        "    output = self.sublayer[2](x, self.feed_forward)\n",
        "    return output\n",
        "\n",
        "''' Generator '''\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab)\n",
        "  def forward(self, x):\n",
        "    out = self.proj(x)\n",
        "    probs = nn.functional.log_softmax(out, dim =-1)\n",
        "    return probs\n",
        "\n",
        "''' Transformer '''\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList(\n",
        "        [deepcopy(layer) for i in range(N)])\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "    output = self.norm(x)\n",
        "    return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "  def encode(self, src, src_mask):\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    return self.decoded(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    memory = self.encode(src, src_mask)\n",
        "    output = self.decode(memory, src_mask, tgt, tgt_mask)"
      ],
      "metadata": {
        "id": "zdS6fREuBIl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Translation"
      ],
      "metadata": {
        "id": "Qu00CEo_WDPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(src_vocab, tgt_vocab, N, d_model, d_ff, h, dropout=0.1):\n",
        "  attn = MultiHeadAttention(h, d_model).to(device)\n",
        "  feedforward = PositionwiseFeedForward(d_model, d_ff, dropout).to(device)\n",
        "  posenc = PositionalEncoding(d_model, dropout).to(device)\n",
        "  model = Transformer(\n",
        "    Encoder(EncoderLayer(d_model,\n",
        "      deepcopy(attn), deepcopy(feedforward), dropout).to(device),N).to(device),\n",
        "    Decoder(DecoderLayer(d_model,\n",
        "      deepcopy(attn), deepcopy(feedforward), dropout).to(device),N).to(device),\n",
        "    nn.Sequential(Embeddings(d_model, src_vocab).to(device), deepcopy(posenc)),\n",
        "    nn.Sequential(Embeddings(d_model, tgt_vocab).to(device), deepcopy(posenc)),\n",
        "    Generator(d_model, tgt_vocab)).to(device)\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "  return model.to(device)\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "\tdef __init__(self, size, padding_idx, smoothing=0.1):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.criterion = nn.KLDivLoss(reduction='sum')\n",
        "\t\tself.padding_idx = padding_idx\n",
        "\t\tself.confidence = 1.0 - smoothing\n",
        "\t\tself.smoothing = smoothing\n",
        "\t\tself.size = size\n",
        "\t\tself.true_dist = None\n",
        "\n",
        "\tdef forward(self, x, target):\n",
        "\t\tassert x.size(1) == self.size\n",
        "\t\ttrue_dist = x.data.clone()\n",
        "\t\ttrue_dist.fill_(self.smoothing / (self.size - 2))\n",
        "\t\ttrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "\t\ttrue_dist[:, self.padding_idx] = 0\n",
        "\t\tmask = torch.nonzero(target.data == self.padding_idx)\n",
        "\t\tif mask.dim() > 0:\n",
        "\t\t\ttrue_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\t\tself.true_dist = true_dist\n",
        "\t\toutput = self.criterion(x, true_dist.clone().detach())\n",
        "\t\treturn output\n",
        "\n",
        "class Optimizer:\n",
        "  def __init__(self, model_size, factor, warmup, optimizer):\n",
        "    self.optimizer = optimizer\n",
        "    self._step = 0\n",
        "    self.warmup = warmup\n",
        "    self.factor = factor\n",
        "    self.model_size = model_size\n",
        "    self._rate = 0\n",
        "  def step(self):\n",
        "    self._step +=1\n",
        "    rate = self.rate()\n",
        "    for p in self.optimizer.param_groups:\n",
        "      p['lr'] = rate\n",
        "    self._rate = rate\n",
        "    self.optimizer.step()\n",
        "  def rate(self, step=None):\n",
        "    if step is None:\n",
        "      step = self._step\n",
        "    output = self.factor * (self.model_size **(-.5) * \\\n",
        "                min(step **(-.5), step * self.warmup **(-1.5)))\n",
        "    return output\n",
        "\n",
        "optimizer = Optimizer(256, 1, 2000, torch.optim.Adam(\n",
        "    model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(\n",
        "        x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
        "    loss.backward()\n",
        "    if self.opt is not None:\n",
        "      self.opt.step()\n",
        "      self.opt.optimizer.zero_grad()\n",
        "    return loss.data.item() * norm.float()\n",
        "\n",
        "criterion = LabelSmoothing(tgt_vocab, padding_idx=0, smoothing=0.1)\n",
        "loss_func = SimpleLossCompute(model.generator, criterion, optimizer)"
      ],
      "metadata": {
        "id": "H77BIbzUBIpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' train the model '''\n",
        "for epoch in range(50):\n",
        "  model.train()\n",
        "  totalloss = 0\n",
        "  tokens = 0\n",
        "  for batch in batches:\n",
        "    out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "    loss = loss_func(out, batch.trg_y, batch.ntokens)\n",
        "    totalloss += loss\n",
        "    tokens += batch.ntokens\n",
        "  print(f'Epoch {epoch}, average loss {totalloss/tokens}')\n",
        "torch.save(model.state_dict(), 'english-to-french.pth')"
      ],
      "metadata": {
        "id": "tk-XZJ3oBIwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(english):\n",
        "  tokenized_en = tokenizer.tokenize(english)\n",
        "  tokenized_en = ['BOS'] + tokenized_en + ['EOS']\n",
        "  en_idx = [en_worddict.get(i,UNK) for i in tokenized_en]\n",
        "  src = torch.tensor(en_idx).long().to(device).unsqueeze(0)\n",
        "  src_mask = (src!=0).unsqueeze(-2)\n",
        "  memory = model.encode(src, src_mask)\n",
        "  start_symbol = fr_worddict['BOS']\n",
        "  ys = torch.ones(1,1).fill_(start_symbol).type_as(src.data)\n",
        "  translation = []\n",
        "  for i in range(50):\n",
        "    out = model.decode(memory, src_mask, ys,\n",
        "                       subsequent_mask(ys.size(1)).type_as(src.data))\n",
        "    prob = model.generator(out[:, -1])\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.data[0]\n",
        "    ys = torch.cat(\n",
        "        [ys, torch.ones(1,1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    sym = fr_idxdict[ys[0, -1].item()]\n",
        "    if sym != 'EOS':\n",
        "      translation.append(sym)\n",
        "    else:\n",
        "      break\n",
        "  translate = ''.join(translation)\n",
        "  translate = translate.replace('</w', ' ')\n",
        "  for x in '''?:;.,'(\"-!&)%''':\n",
        "    translate = translate.replace(f' {x}, f'{x})\n",
        "  print(translate)\n",
        "  return translate"
      ],
      "metadata": {
        "id": "2BYxNK8RBIz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}