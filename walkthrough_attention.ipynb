{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer_:\n",
        "  def __init__(self, vocab):\n",
        "    self.dict_word_to_idx = vocab\n",
        "    self.dict_idx_to_word = {idx:word for word, idx in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [word.strip() for word in preprocessed if word.strip()]\n",
        "    preprocessed = [word if word in self.dict_word_to_idx\n",
        "                         else \"<|unk|>\" for word in preprocessed]\n",
        "    idx_lst = [self.dict_word_to_idx[word] for word in preprocessed]\n",
        "    return idx_lst\n",
        "\n",
        "  def decode(self, idx_lst):\n",
        "    text = \" \".join([self.dict_idx_to_word[idx] for idx in idx_lst])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "simpleClass = Tokenizer_(vocab)"
      ],
      "metadata": {
        "id": "-4d8UU3Fpaza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and Dataloader"
      ],
      "metadata": {
        "id": "Hb9tD1YgtLEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDatasertV1(Dataset):\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    # tokenize the entire text\n",
        "    tokens_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    # use a sliding window to chunk and book into overlapping sequences of max length\n",
        "    for i in range(0, len(tokens_ids) -max_length, stride):\n",
        "      input_chunk = tokens_ids[i:i+max_length]\n",
        "      target_chunk = tokens_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "5AdI-fVVrbfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GPTDataloader(text, batch_size=4, max_length=256, stride=128, shuffle=True,\n",
        "                    drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "  dataset = GPTDatasertV1(text, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset, batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers)\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "C9yvu91rrbjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader_ = GPTDataloader(\n",
        "    m_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader_)\n",
        "inputs, targets = next(data_iter)\n",
        "inputs, inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLLVEYbcyO6f",
        "outputId": "c105f711-ab16-430d-8d2a-2bfb4270e6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   40,   367,  2885,  1464],\n",
              "         [ 1807,  3619,   402,   271],\n",
              "         [10899,  2138,   257,  7026],\n",
              "         [15632,   438,  2016,   257],\n",
              "         [  922,  5891,  1576,   438],\n",
              "         [  568,   340,   373,   645],\n",
              "         [ 1049,  5975,   284,   502],\n",
              "         [  284,  3285,   326,    11]]),\n",
              " torch.Size([8, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "token_embeddings.shape, #token_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8RnE9WbyO-t",
        "outputId": "d4dc8849-f2d3-4b11-811f-32182f60c425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 4, 256]),)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embedding_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--nTUYDbyPDm",
        "outputId": "7d4b52d1-0a31-42c2-e11c-da30949d310e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(4, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "pos_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8Ma1uU0XEQ",
        "outputId": "79ae6aec-d5a3-4020-adc6-d7a9c4bf3b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
              "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
              "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
              "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "input_embeddings.shape, #input_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xyupzkMvI1m",
        "outputId": "3492af75-8c6e-48c3-8661-919649be7601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 4, 256]),)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Attention"
      ],
      "metadata": {
        "id": "Uaatx0RM05c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query  = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.W_key    = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.W_value  = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    query = self.W_query(x)\n",
        "\n",
        "    attention_scores = query @ keys.T\n",
        "    attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vectors = attention_weights @ values\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "RbaeS6pc82UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "self_attention_v2 = SelfAttentionV2(dim_in=3, dim_out=2)\n",
        "self_attention_v2,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROZkfmdf82YK",
        "outputId": "64bfe9c5-4098-492d-c500-7107d9d064b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(SelfAttentionV2(\n",
              "   (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
              "   (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
              "   (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
              " ),)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_attention_v2(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmng-T0x82k6",
        "outputId": "d6451d9b-ac2c-4ecc-e3e9-f621e516347b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0739,  0.0713],\n",
              "        [-0.0748,  0.0703],\n",
              "        [-0.0749,  0.0702],\n",
              "        [-0.0760,  0.0685],\n",
              "        [-0.0763,  0.0679],\n",
              "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masking additional attention with dropout"
      ],
      "metadata": {
        "id": "uYToHxT3Cymm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' implementation of a causal self attention and multi head attention '''"
      ],
      "metadata": {
        "id": "72STk5D9C1KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out, context_length,\n",
        "               dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.dim_in = dim_in\n",
        "    self.dim_out = dim_out\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.W_query  = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.W_key    = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.W_value  = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.register_buffer('mask', torch.triu(\n",
        "        torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, dim_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    query = self.W_query(x)\n",
        "\n",
        "    attention_scores = query @ keys.transpose(1,2)\n",
        "    attention_scores = attention_scores.masked_fill(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attention_weights = torch.softmax(\n",
        "        attention_scores/keys.shape[-1]**0.5, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    context_vectors = attention_weights @ values\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "YHkdt6_y3ImR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self,\n",
        "        dim_in, dim_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "      super().__init__()\n",
        "      self.heads = nn.ModuleList([CausalAttention(\n",
        "                          dim_in, dim_out, context_length, dropout, qkv_bias)\n",
        "                      for i in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "      return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "Ahu3MYmB3I2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "mha = MultiHeadAttentionWrapper(dim_in, dim_out, context_length, 0.0, num_heads=1)\n",
        "context_vecs = mha(batch)\n",
        "mha, context_vecs, context_vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek7twEb33I8a",
        "outputId": "a8308c07-6c8d-4d5a-ae35-323d292e4106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MultiHeadAttentionWrapper(\n",
              "   (heads): ModuleList(\n",
              "     (0): CausalAttention(\n",
              "       (dropout): Dropout(p=0.0, inplace=False)\n",
              "       (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
              "       (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
              "       (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
              "     )\n",
              "   )\n",
              " ),\n",
              " tensor([[[-0.4519,  0.2216],\n",
              "          [-0.5874,  0.0058],\n",
              "          [-0.6300, -0.0632],\n",
              "          [-0.5675, -0.0843],\n",
              "          [-0.5526, -0.0981],\n",
              "          [-0.5299, -0.1081]],\n",
              " \n",
              "         [[-0.4519,  0.2216],\n",
              "          [-0.5874,  0.0058],\n",
              "          [-0.6300, -0.0632],\n",
              "          [-0.5675, -0.0843],\n",
              "          [-0.5526, -0.0981],\n",
              "          [-0.5299, -0.1081]]], grad_fn=<CatBackward0>),\n",
              " torch.Size([2, 6, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-head attention with weight splits"
      ],
      "metadata": {
        "id": "Kk_Pv5IbBKHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "          dim_in, dim_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert(dim_out % num_heads ==0), 'dim_out must be divisible by num_heads'\n",
        "    self.dim_in = dim_in\n",
        "    self.dim_out = dim_out\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = dim_out // num_heads\n",
        "\n",
        "    self.W_query  = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.W_key    = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.W_value  = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(dim_out, dim_out)\n",
        "    self.register_buffer('mask',\n",
        "        torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, dim_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    query = self.W_query(x)\n",
        "\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    query = query.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    query = query.transpose(1,2)\n",
        "\n",
        "    attention_scores = query @ keys.transpose(2,3)\n",
        "    attention_scores = attention_scores.masked_fill(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "\n",
        "    attention_weights = torch.softmax(\n",
        "        attention_scores/keys.shape[-1]**0.5, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    context_vectors = (attention_weights @ values).transpose(1,2)\n",
        "    context_vectors = context_vectors.contiguous().view(b, num_tokens, self.dim_out)\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "yDBHDOfj3Jmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_size, context_length, dim_in = batch.shape\n",
        "dim_out = 2\n",
        "\n",
        "mha = MultiHeadAttention(dim_in, dim_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "mha, context_vecs, context_vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frn0LXFkDPwP",
        "outputId": "e2fdc026-d6e3-4a12-a808-b3c8e07e85a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MultiHeadAttention(\n",
              "   (dropout): Dropout(p=0.0, inplace=False)\n",
              "   (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
              "   (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
              "   (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
              "   (out_proj): Linear(in_features=2, out_features=2, bias=True)\n",
              " ),\n",
              " tensor([[[0.3190, 0.4858],\n",
              "          [0.2943, 0.3897],\n",
              "          [0.2856, 0.3593],\n",
              "          [0.2693, 0.3873],\n",
              "          [0.2639, 0.3928],\n",
              "          [0.2575, 0.4028]],\n",
              " \n",
              "         [[0.3190, 0.4858],\n",
              "          [0.2943, 0.3897],\n",
              "          [0.2856, 0.3593],\n",
              "          [0.2693, 0.3873],\n",
              "          [0.2639, 0.3928],\n",
              "          [0.2575, 0.4028]]], grad_fn=<ViewBackward0>),\n",
              " torch.Size([2, 6, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids, tokenizer, #text_to_token_ids(token_ids, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9sFp9rUoOo",
        "outputId": "94bdc581-21f7-43d5-b5f2-990264785e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 6109,  3626,  6100,   345, 14504]]), <Encoding 'gpt2'>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}