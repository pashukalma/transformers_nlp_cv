{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI9GhfnUW8tW",
        "outputId": "e721bb89-645f-4036-c61d-0367d7eb8096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.5.1+cu124', '0.20.1+cu124')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "torch.__version__, torchvision.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a15CABymkX1J"
      },
      "source": [
        "##### nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZYVXNZlW_Dp",
        "outputId": "ce668010-f9bf-4921-eee1-6a85026b473c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['the', 'project', 'gutenberg', 'ebook', 'of'],\n",
              " [',', '.', 'the', 'and', 'to', 'of', 'he', '‚Äù', 'a', 'in'],\n",
              " 15122)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' text processing '''\n",
        "with open('guttenberg.txt', 'r') as fd:\n",
        "  text = fd.read()\n",
        "words = text.split(' ')\n",
        "text = text.lower()\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('-', ' ')\n",
        "for char in \",.:;?!$_@&#*%\":\n",
        "  text = text.replace(f'{char}', f' {char} ')\n",
        "text = text.replace('\"', ' \" ')\n",
        "text = text.split()\n",
        "from collections import Counter\n",
        "wordscount = Counter(text)\n",
        "words = sorted(wordscount, key=wordscount.get, reverse=True)\n",
        "word_to_int = {value:key for key, value in enumerate(words)}\n",
        "int_to_word = {key:value for key, value in enumerate(words)}\n",
        "word_index = [word_to_int[word] for word in text]\n",
        "text[:5], words[:10], len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xqpUoTOkbXn"
      },
      "outputs": [],
      "source": [
        "''' work with the batch of training data '''\n",
        "seqlength = 100\n",
        "substr = []\n",
        "for n in range(0, len(word_index)-seqlength-1):\n",
        "  t = word_index[n: n+seqlength]\n",
        "  t1 = word_index[n+1: n+seqlength+1]\n",
        "  substr.append((torch.tensor(t), (torch.tensor(t1))))\n",
        "\n",
        "torch.manual_seed(40)\n",
        "batchsize=32\n",
        "loader = DataLoader(substr, batch_size=batchsize, shuffle=True)\n",
        "x, y = next(iter(loader))\n",
        "x.shape, y.shape, x[:1], y[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrzkHPATkbqf",
        "outputId": "6ffd3e5e-756e-441f-b01e-88f4a3c767e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(15122, 128)\n",
              "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
              "  (fc): Linear(in_features=128, out_features=15122, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' define and train the model '''\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size=128, n_embed=128, n_layers=3, drop_prob=0.2):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_embed = n_embed\n",
        "    vocab_size = len(word_to_int)\n",
        "    self.embedding = nn.Embedding(vocab_size, n_embed)\n",
        "    self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.n_embed,\n",
        "          num_layers=self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "    self.fc = nn.Linear(input_size, vocab_size)\n",
        "  def forward(self, x, hc):\n",
        "    embed = self.embedding(x)\n",
        "    x, hc = self.lstm(embed, hc)\n",
        "    x = self.fc(x)\n",
        "    return x, hc\n",
        "  def init_hidden(self, n_seqs):\n",
        "    weight = next(self.parameters()).data\n",
        "    return (weight.new(self.n_layers, n_seqs, self.n_embed).zero_(),\n",
        "            weight.new(self.n_layers, n_seqs, self.n_embed).zero_())\n",
        "\n",
        "model = LSTM().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "for epoch in range(1):\n",
        "  totalloss = 0\n",
        "  sh, sc = model.init_hidden(batchsize)\n",
        "  for i, (x,y) in enumerate(loader):\n",
        "    if x.shape[0] == batchsize:\n",
        "      inputs, targets = x.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output, (sh, sc) = model(inputs, (sh,sc))\n",
        "      loss = loss_func(output.transpose(1,2), targets)\n",
        "      sh, sc = sh.detach(), sc.detach()\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm(model.parameters(), 5)\n",
        "      optimizer.step()\n",
        "      totalloss += loss.item()\n",
        "    if (i+1)%1000 ==0:\n",
        "      print(f'at epoch {epoch} iteration {i+1} average loss = {totalloss/(i+1)}')"
      ],
      "metadata": {
        "id": "NngZrF8jwd_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' generate text and top-K sampling '''\n",
        "def sample(model, prompt, length=100):\n",
        "  model.eval()\n",
        "  text = prompt.lower().split(' ')\n",
        "  hc = model.init_hidden(1)\n",
        "  length = length - len(text)\n",
        "  for i in range(0, length):\n",
        "    if len(text) <=seqlength:\n",
        "      x = torch.tensor([[word_to_int[word] for word in text]])\n",
        "    else:\n",
        "      x = torch.tensor([[word_to_int[word] for word in text[-seqlength:]]])\n",
        "    inputs = x.to(device)\n",
        "    output, hc = model(inputs, hc)\n",
        "    logits = output[0][-1]\n",
        "    p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
        "    idx = np.random.choice(len(logits), p=p)\n",
        "  text = ' '.join(text)\n",
        "  return text\n",
        "sample(model, prompt='project guttenberg')"
      ],
      "metadata": {
        "id": "227w5gjTijLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2DkIFth8lz9"
      },
      "outputs": [],
      "source": [
        "''' top-K sampling '''\n",
        "def generate(model, prompt, top_k=None, length=100, temperature=1):\n",
        "  model.eval()\n",
        "  text = prompt.lower().split(' ')\n",
        "  hc = model.init_hidden(1)\n",
        "  length = length - len(text)\n",
        "  for i in range(0, length):\n",
        "    if len(text) <=seqlength:\n",
        "      x = torch.tensor([[word_to_int[word] for word in text]])\n",
        "    else:\n",
        "      x = torch.tensor([[word_to_int[word] for word in text[-seqlength:]]])\n",
        "    inputs = x.to(device)\n",
        "    output, hc = model(inputs, hc)\n",
        "    logits = output[0][-1]\n",
        "    logits = logits/temperature\n",
        "    p = nn.functional.softmax(logits, dim=0).detach().cpu()\n",
        "    if top_k is None:\n",
        "      idx = np.random.choice(len(logits), p=p.numpy())\n",
        "    else:\n",
        "      ps, tops = p.topk(top_k)\n",
        "      ps = ps/ps.sum()\n",
        "      idx = np.random.choice(tops, p=ps.numpy())\n",
        "    text.append(int_to_word[idx])\n",
        "    text = ' '.join(text)\n",
        "  return text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}